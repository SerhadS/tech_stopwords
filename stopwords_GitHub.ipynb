{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to reproduce the findings in the paper:\n",
    "S.Sarica & J.Luo. Stopwords in Technical Language Processing\n",
    "\n",
    "All the files can be found in following dropbox folder:\n",
    "https://www.dropbox.com/sh/hsuum451kyhp2km/AAD49aUd3ut_xICj0WRoG2rIa?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "temp = []\n",
    "for i in range(len(punct)):\n",
    "    if punct[i]=='-' or punct[i]=='/':\n",
    "        temp.append(i)\n",
    "temp = sorted(temp, reverse=True)\n",
    "for i in temp:\n",
    "    punct = punct[:i]+punct[i+1:]\n",
    "translator = str.maketrans('', '', punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK and USPTO Stopwords\n",
    "nltk_stops = set(stopwords.words('english'))\n",
    "USPTO_stops = []\n",
    "\n",
    "with open ('./data/USPTO_stopwords.csv') as f:\n",
    "    sreader = csv.reader(f)\n",
    "    USPTO_stops = [x[0] for x in sreader.readlines()]\n",
    "set_stops = nltk_stops.union(set(USPTO_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## token stats w/o nltk+USPTO stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(key, vocab):\n",
    "    ent = vocab[key]['docs']\n",
    "    sum_ = sum(ent)\n",
    "    entropy = 0\n",
    "    for x in ent:\n",
    "        entropy += x/sum_*np.log(x/sum_)\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './stopwords_data/'\n",
    "\n",
    "vocab = {}\n",
    "patent_num = 0\n",
    "with open(data_folder + 'patents_titles_abstracts_line_sentence_preprocessed.txt',\n",
    "          encoding = 'utf-8') as f1:\n",
    "    with open(data_folder + 'line_sentence_patentnumbers.txt',\n",
    "              encoding = 'utf-8') as f2:\n",
    "        pat_no = ''\n",
    "        flag = 0\n",
    "        temp = []\n",
    "        count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                temp1 = next(f1)[:-1]\n",
    "                temp2 = next(f2)[:-1]\n",
    "                if temp2!= pat_no:\n",
    "                    patent_num += 1\n",
    "                    temp = [x for x in temp if x not in punct]\n",
    "                    counts = Counter(temp)\n",
    "                    for key in counts.keys():\n",
    "                        if vocab.get(key, False) == False:\n",
    "                            vocab[key] = {'docs':[], 'count':0, 'tf_doc':[]}\n",
    "                        vocab[key]['docs'] += [counts[key]]\n",
    "                        vocab[key]['count'] += counts[key]\n",
    "                        vocab[key]['tf_doc'] +=[counts[key]/len(temp)]\n",
    "\n",
    "                    pat_no = temp2\n",
    "                    temp = []                \n",
    "                temp += word_tokenize(temp1)\n",
    "                count+=1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(count, e)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in vocab.keys():\n",
    "    if vocab[key].get('tfidf',-1)==-1:\n",
    "        vocab[key]['idf'] = np.log(patent_num/len(vocab[key]['tf_doc']))\n",
    "        vocab[key]['tfidf'] = 1/len(vocab[key]['docs'])*\\\n",
    "                                    sum(vocab[key]['tf_doc'])*patent_num/len(vocab[key]['tf_doc'])\n",
    "        vocab[key]['entropy'] = entropy(key, vocab)                         \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by decreasing entropy\n",
    "sorted_entropy_vocab = sorted([(item[0],item[1]['entropy']) for item in vocab.items()\\\n",
    "                               if item[0] not in set_stops and item[1]['count']>1],\n",
    "                              key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by increasing modified tfidf\n",
    "sorted_tfidf_vocab = sorted([(item[0],item[1]['tfidf']) for item in vocab.items()\\\n",
    "                             if item[0] not in set_stops and item[1]['count']>1], \n",
    "                            key = lambda x:x[1], reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by decreasing term count\n",
    "sorted_f_vocab = sorted([(item[0],item[1]['count']) for item in vocab.items()\\\n",
    "                         if item[0] not in set_stops and item[1]['count']>1], \n",
    "                        key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by decreasing term count\n",
    "sorted_idf_vocab = sorted([(item[0],item[1]['idf']) for item in vocab.items()\\\n",
    "                         if item[0] not in set_stops and item[1]['count']>1], \n",
    "                        key = lambda x:x[1], reverse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### technical stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + 'technical_stopwords.txt') as f:\n",
    "    tech_stops = f.readlines()\n",
    "tech_stops = set([x.strip() for x in tech_stops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly selected patents from three different CPC subgroups for each CPC section\n",
    "with open(data_folder + 'random_patents_topic_modelling.pkl', 'rb') as f:\n",
    "    random_patents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected CPC subgroups\n",
    "select_cpc = [\"A01K\", \"B01D\", \"C06B\", \"D21F\", \"E01H\", \"F02B\", \"G06F\", \"H04B\"]\n",
    "#labels\n",
    "labs = [int(i/100) for i in range(800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents = []\n",
    "for cpc in select_cpc:\n",
    "    patents+=random_patents[cpc]\n",
    "patents = sorted(patents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading patent texts from preprocessed file\n",
    "\n",
    "pats__ = {}\n",
    "pats__ = {num:\"\" for num in patents}\n",
    "with open(data_folder + 'patents_titles_abstracts_line_sentence_preprocessed.txt',\n",
    "          'r', encoding = 'utf-8') as f1:\n",
    "    with open(data_folder + 'line_sentence_patentnumbers.txt', 'r',\n",
    "              encoding = 'utf-8') as f2:\n",
    "        c = 0\n",
    "        while True:\n",
    "            try:\n",
    "                temp = next(f1).strip()\n",
    "                temp_num = next(f2).strip()\n",
    "            except:\n",
    "                break\n",
    "            c += 1\n",
    "            if temp_num.isdigit():\n",
    "                temp_num = int(temp_num)\n",
    "                if temp_num in pats__.keys() and pats__[temp_num] == '':\n",
    "                    pats__[temp_num] += temp + ' '\n",
    "pats__ = {key:value for key, value in pats__.items() if value}\n",
    "\n",
    "#remove the patents which do not have any stopwords\n",
    "#we need this to measure the effectiveness of filtering stopwords\n",
    "#for topic modelling tasks\n",
    "\n",
    "to_pop = []\n",
    "for key,value in pats__.items():\n",
    "    if not any(x in word_tokenize(value) for x in tech_stops):\n",
    "        to_pop.append(key)\n",
    "       \n",
    "for key in to_pop:\n",
    "    pats__.pop(key)\n",
    "    \n",
    "    \n",
    "#creating the final patents list to be randomly selected from\n",
    "pats_secs = [[] for x in range(8)]\n",
    "for i,cpc in enumerate(select_cpc):\n",
    "    for pat in random_patents[cpc]:\n",
    "        if pats__.get(pat):\n",
    "            pats_secs[i].append(pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling with NMF and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmis = []\n",
    "nmis_ = []\n",
    "\n",
    "#nmis saves the normalized mutual information measurements for NMF based topic modelling\n",
    "#while nmis_ saves the same for LDA based topic modelling\n",
    "\n",
    "for i in range(1000):\n",
    "    temp_pats = []\n",
    "    for sec in pats_secs:\n",
    "        temp_pats += random.sample(sec, 100)\n",
    "    temp_vocab = {}\n",
    "    temp_vocab_1 = {}\n",
    "    temp_vocab_2 = {}\n",
    "    ind = 0\n",
    "    ind_1 = 0\n",
    "    ind_2 = 0\n",
    "    for key in temp_pats:\n",
    "        if pats__.get(key):\n",
    "            value = pats__[key]\n",
    "            for sent in sent_tokenize(value.strip()):\n",
    "                for word in word_tokenize(sent):\n",
    "                    if word not in temp_vocab.keys() and word not in punct:\n",
    "                        temp_vocab[word] = ind\n",
    "                        ind += 1\n",
    "                    if word not in temp_vocab_1.keys() and word not in punct and word not in set_stops:\n",
    "                        temp_vocab_1[word] = ind_1\n",
    "                        ind_1 +=1\n",
    "                    if word not in temp_vocab_2.keys() and word not in punct and word not in set_stops.union(tech_stops):\n",
    "                        temp_vocab_2[word] = ind_2\n",
    "                        ind_2 +=1\n",
    "    tfidf_vectorizer_ = TfidfVectorizer(vocabulary = temp_vocab)\n",
    "    tfidf_ = tfidf_vectorizer_.fit_transform([pats__[x] for x in temp_pats])\n",
    "    tfidf_vectorizer_1_ = TfidfVectorizer(vocabulary = temp_vocab_1)\n",
    "    tfidf_1_ = tfidf_vectorizer_1_.fit_transform([pats__[x] for x in temp_pats])\n",
    "    tfidf_vectorizer_2_ = TfidfVectorizer(vocabulary = temp_vocab_2)\n",
    "    tfidf_2_ = tfidf_vectorizer_2_.fit_transform([pats__[x] for x in temp_pats])\n",
    "    nmf_ = NMF(n_components=8, random_state=1, alpha=.1,\n",
    "          l1_ratio=.5, init='nndsvd').fit(tfidf_)\n",
    "    nmf_1_ = NMF(n_components=8, random_state=1, alpha=.1,\n",
    "          l1_ratio=.5, init='nndsvd').fit(tfidf_1_)\n",
    "    nmf_2_ = NMF(n_components=8, random_state=1, alpha=.1,\n",
    "          l1_ratio=.5, init='nndsvd').fit(tfidf_2_)\n",
    "    \n",
    "    nmf_labels_ = [np.argmax(x) for x\\\n",
    "            in nmf_.transform(tfidf_vectorizer_.transform([pats__[x] for x in temp_pats]))]\n",
    "    nmf_labels_1_ = [np.argmax(x) for x\\\n",
    "            in nmf_1_.transform(tfidf_vectorizer_1_.transform([pats__[x] for x in temp_pats]))]\n",
    "    nmf_labels_2_ = [np.argmax(x) for x\\\n",
    "            in nmf_2_.transform(tfidf_vectorizer_2_.transform([pats__[x] for x in temp_pats]))]\n",
    "    nmis.append([nmi(labs, nmf_labels_), \n",
    "                 nmi(labs, nmf_labels_1_),\n",
    "                 nmi(labs, nmf_labels_2_)])\n",
    "    print(f'NMF:{nmis[-1]}')\n",
    "    tf_vectorizer_ = CountVectorizer(vocabulary = temp_vocab)\n",
    "    tf_ = tf_vectorizer_.fit_transform([pats__[x] for x in temp_pats])\n",
    "    tf_vectorizer_1_ = CountVectorizer(vocabulary = temp_vocab_1)\n",
    "    tf_1_ = tf_vectorizer_1_.fit_transform([pats__[x] for x in temp_pats])\n",
    "    tf_vectorizer_2_ = CountVectorizer(vocabulary = temp_vocab_2)\n",
    "    tf_2_ = tf_vectorizer_2_.fit_transform([pats__[x] for x in temp_pats])\n",
    "    lda_ = LatentDirichletAllocation(n_components=8, max_iter=200, \n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0).fit(tf_)\n",
    "    lda_1_ = LatentDirichletAllocation(n_components=8, max_iter=200, \n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0).fit(tf_1_)\n",
    "    lda_2_ = LatentDirichletAllocation(n_components=8, max_iter=200, \n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0).fit(tf_2_)\n",
    "    \n",
    "    lda_labels_ = [np.argmax(x) for x\\\n",
    "            in lda_.transform(tf_vectorizer_.transform([pats__[x] for x in temp_pats]))]\n",
    "    lda_labels_1_ = [np.argmax(x) for x\\\n",
    "            in lda_1_.transform(tf_vectorizer_1_.transform([pats__[x] for x in temp_pats]))]\n",
    "    lda_labels_2_ = [np.argmax(x) for x\\\n",
    "            in lda_2_.transform(tf_vectorizer_2_.transform([pats__[x] for x in temp_pats]))]\n",
    "    nmis_.append([nmi(labs, lda_labels_), \n",
    "                 nmi(labs, lda_labels_1_),\n",
    "                 nmi(labs, lda_labels_2_)])\n",
    "    print(f'LDA:{nmis_[-1]}\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
