{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to reproduce the findings in the paper:\n",
    "S.Sarica & J.Luo. Stopwords in Technical Language Processing\n",
    "\n",
    "All the files can be found in following dropbox folder:\n",
    "https://www.dropbox.com/sh/hsuum451kyhp2km/AAD49aUd3ut_xICj0WRoG2rIa?dl=0\n",
    "\n",
    "#### !!! These files should be copied under ./data folder. !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "temp = []\n",
    "for i in range(len(punct)):\n",
    "    if punct[i]=='-' or punct[i]=='/':\n",
    "        temp.append(i)\n",
    "temp = sorted(temp, reverse=True)\n",
    "for i in temp:\n",
    "    punct = punct[:i]+punct[i+1:]\n",
    "translator = str.maketrans('', '', punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK and USPTO Stopwords\n",
    "nltk_stops = set(stopwords.words('english'))\n",
    "USPTO_stops = []\n",
    "\n",
    "with open (data_folder+'USPTO_stopwords.csv') as f:\n",
    "    sreader = csv.reader(f)\n",
    "    USPTO_stops = [x[0] for x in sreader]\n",
    "set_stops = nltk_stops.union(set(USPTO_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## token stats w/o nltk+USPTO stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(key, vocab):\n",
    "    ent = vocab[key]['docs']\n",
    "    sum_ = sum(ent)\n",
    "    entropy = 0\n",
    "    for x in ent:\n",
    "        entropy += x/sum_*np.log(x/sum_)\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "patent_num = 0\n",
    "with open(data_folder + 'patents_titles_abstracts_line_sentence_preprocessed.txt',\n",
    "          encoding = 'utf-8') as f1:\n",
    "    with open(data_folder + 'line_sentence_patentnumbers.txt',\n",
    "              encoding = 'utf-8') as f2:\n",
    "        pat_no = ''\n",
    "        flag = 0\n",
    "        temp = []\n",
    "        count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                temp1 = next(f1)[:-1]\n",
    "                temp2 = next(f2)[:-1]\n",
    "                if temp2!= pat_no:\n",
    "                    patent_num += 1\n",
    "                    temp = [x for x in temp if x not in punct]\n",
    "                    counts = Counter(temp)\n",
    "                    for key in counts.keys():\n",
    "                        if vocab.get(key, False) == False:\n",
    "                            vocab[key] = {'docs':[], 'count':0, 'tf_doc':[]}\n",
    "                        vocab[key]['docs'] += [counts[key]]\n",
    "                        vocab[key]['count'] += counts[key]\n",
    "                        vocab[key]['tf_doc'] +=[counts[key]/len(temp)]\n",
    "\n",
    "                    pat_no = temp2\n",
    "                    temp = []                \n",
    "                temp += word_tokenize(temp1)\n",
    "                count+=1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(count, e)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in vocab.keys():\n",
    "    if vocab[key].get('tfidf',-1)==-1:\n",
    "        vocab[key]['idf'] = np.log(patent_num/len(vocab[key]['tf_doc']))\n",
    "        vocab[key]['tfidf'] = 1/len(vocab[key]['docs'])*\\\n",
    "                                    sum(vocab[key]['tf_doc'])*patent_num/len(vocab[key]['tf_doc'])\n",
    "        vocab[key]['entropy'] = entropy(key, vocab)                         \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by decreasing entropy\n",
    "sorted_entropy_vocab = sorted([(item[0],item[1]['entropy']) for item in vocab.items()\\\n",
    "                               if item[0] not in set_stops and item[1]['count']>1],\n",
    "                              key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by increasing modified tfidf\n",
    "sorted_tfidf_vocab = sorted([(item[0],item[1]['tfidf']) for item in vocab.items()\\\n",
    "                             if item[0] not in set_stops and item[1]['count']>1], \n",
    "                            key = lambda x:x[1], reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by decreasing term count\n",
    "sorted_f_vocab = sorted([(item[0],item[1]['count']) for item in vocab.items()\\\n",
    "                         if item[0] not in set_stops and item[1]['count']>1], \n",
    "                        key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary sorted by decreasing term count\n",
    "sorted_idf_vocab = sorted([(item[0],item[1]['idf']) for item in vocab.items()\\\n",
    "                         if item[0] not in set_stops and item[1]['count']>1], \n",
    "                        key = lambda x:x[1], reverse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### technical stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + 'technical_stopwords.txt') as f:\n",
    "    tech_stops = f.readlines()\n",
    "tech_stops = set([x.strip().replace(' ', '_') for x in tech_stops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT CLASSIFICATION w/ LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly selected patents from three different CPC subgroups for each CPC section\n",
    "with open(data_folder + 'random_patents_topic_modelling.pkl', 'rb') as f:\n",
    "    random_patents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected CPC subgroups\n",
    "select_cpc = [\"A01K\", \"B01D\", \"C06B\", \"D21F\", \"E01H\", \"F02B\", \"G06F\", \"H04B\"]\n",
    "#labels\n",
    "labs = [int(i/100) for i in range(800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents = []\n",
    "for cpc in select_cpc:\n",
    "    patents+=random_patents[cpc]\n",
    "patents = sorted(patents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading patent texts from preprocessed file\n",
    "\n",
    "pats__ = {}\n",
    "pats__ = {num:\"\" for num in patents}\n",
    "with open(data_folder + 'patents_titles_abstracts_line_sentence_preprocessed.txt',\n",
    "          'r', encoding = 'utf-8') as f1:\n",
    "    with open(data_folder + 'line_sentence_patentnumbers.txt', 'r',\n",
    "              encoding = 'utf-8') as f2:\n",
    "        c = 0\n",
    "        while True:\n",
    "            try:\n",
    "                temp = next(f1).strip()\n",
    "                temp_num = next(f2).strip()\n",
    "            except:\n",
    "                break\n",
    "            c += 1\n",
    "            if temp_num.isdigit():\n",
    "                temp_num = int(temp_num)\n",
    "                if temp_num in pats__.keys() and pats__[temp_num] == '':\n",
    "                    pats__[temp_num] += temp + ' '\n",
    "pats__ = {key:value for key, value in pats__.items() if value}\n",
    "\n",
    "#remove the patents which do not have any stopwords\n",
    "#we need this to measure the effectiveness of filtering stopwords\n",
    "#for topic modelling tasks\n",
    "\n",
    "to_pop = []\n",
    "for key,value in pats__.items():\n",
    "    if not any(x in word_tokenize(value) for x in tech_stops):\n",
    "        to_pop.append(key)\n",
    "       \n",
    "for key in to_pop:\n",
    "    pats__.pop(key)\n",
    "    \n",
    "    \n",
    "#creating the final patents list to be randomly selected from\n",
    "pats_secs = [[] for x in range(8)]\n",
    "for i,cpc in enumerate(select_cpc):\n",
    "    for pat in random_patents[cpc]:\n",
    "        if pats__.get(pat):\n",
    "            pats_secs[i].append(pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_secs_flatten = [item for sublist in pats_secs for item in sublist]\n",
    "texts = [pats__[x] for x in pats_secs_flatten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text for second and third models (removing stopwords from the text)\n",
    "texts_1 = [\" \".join([x for x in word_tokenize(y) if x not in set_stops]) for y in texts]\n",
    "texts_2 = [\" \".join([x for x in word_tokenize(y) if x not in set_stops+tech_stops]) for y in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters=punct, lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = tokenizer.texts_to_sequences(texts_1)\n",
    "X1 = pad_sequences(X1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = tokenizer.texts_to_sequences(texts_2)\n",
    "X2 = pad_sequences(X2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true labels\n",
    "Y = []\n",
    "for i,y in enumerate(pats_secs):\n",
    "    for t in range(len(y)):\n",
    "        Y.append([1 if i == x else 0 for x in range(8)])\n",
    "Y= np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing train and test samples\n",
    "#randomly selected 100 from each CPC goes to test set\n",
    "sum_ = 0\n",
    "X_test = []\n",
    "X_train = []\n",
    "Y_test = []\n",
    "Y_train = []\n",
    "X1_test = []\n",
    "X1_train = []\n",
    "\n",
    "X2_test = []\n",
    "X2_train = []\n",
    "\n",
    "for x in counts:\n",
    "    rand_indices = random.sample(list(range(sum_, sum_+x)), int(x/10))\n",
    "    test_indices = random.sample(list(range(sum_, sum_+x)), 100)\n",
    "    Y_test += [Y[i] for i in test_indices]\n",
    "    Y_train += [Y[i] for i in range(sum_, sum_+x) if i not in rand_indices]\n",
    "    X_test += [X[i] for i in test_indices]\n",
    "    X_train += [X[i] for i in range(sum_, sum_+x) if i not in rand_indices]\n",
    "    X1_test += [X1[i] for i in test_indices]\n",
    "    X1_train += [X1[i] for i in range(sum_, sum_+x) if i not in rand_indices]\n",
    "    X2_test += [X2[i] for i in test_indices]\n",
    "    X2_train += [X2[i] for i in range(sum_, sum_+x) if i not in rand_indices]\n",
    "    \n",
    "    sum_ += x\n",
    "X_test = np.array(X_test)\n",
    "X_train = np.array(X_train)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_train = np.array(Y_train)\n",
    "X1_test = np.array(X1_test)\n",
    "X1_train = np.array(X1_train)\n",
    "X2_test = np.array(X2_test)\n",
    "X2_train = np.array(X2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for the train set with raw texts\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model can be loaded using following line instead of training\n",
    "model = keras.models.load_model(data_folder+'model0_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "541/541 [==============================] - 946s 2s/step - loss: 1.3021 - accuracy: 0.5392 - val_loss: 0.8420 - val_accuracy: 0.6261\n",
    "\n",
    "Epoch 2/5\n",
    "541/541 [==============================] - 975s 2s/step - loss: 0.5993 - accuracy: 0.8224 - val_loss: 0.6938 - val_accuracy: 0.7982\n",
    "\n",
    "Epoch 3/5\n",
    "541/541 [==============================] - 854s 2s/step - loss: 0.4694 - accuracy: 0.8681 - val_loss: 0.5783 - val_accuracy: 0.8323\n",
    "\n",
    "Epoch 4/5\n",
    "541/541 [==============================] - 946s 2s/step - loss: 0.3172 - accuracy: 0.9107 - val_loss: 0.5560 - val_accuracy: 0.8521\n",
    "\n",
    "Epoch 5/5\n",
    "541/541 [==============================] - 920s 2s/step - loss: 0.4282 - accuracy: 0.8794 - val_loss: 0.7425 - val_accuracy: 0.7805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [np.argmax(x) for x in model.predict(X_test)]\n",
    "y_test = [np.argmax(x) for x in Y_test]\n",
    "print(metrics.classification_report(y_test, Y_pred, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0      0.779     0.950     0.856       100\n",
    "           1      0.600     0.990     0.747       100\n",
    "           2      1.000     0.800     0.889       100\n",
    "           3      0.962     0.750     0.843       100\n",
    "           4      0.985     0.650     0.783       100\n",
    "           5      0.952     0.790     0.863       100\n",
    "           6      0.852     0.980     0.912       100\n",
    "           7      0.967     0.880     0.921       100\n",
    "    accuracy                          0.849       800\n",
    "    macro avg     0.887     0.849     0.852       800\n",
    "    weighted avg  0.887     0.849     0.852       800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for the train set without NLTK+USPTO stopwords\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X1.shape[1]))\n",
    "model1.add(SpatialDropout1D(0.2))\n",
    "model1.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model1.add(Dense(8, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model can be loaded using following line instead of training\n",
    "model1 = keras.models.load_model(data_folder+'model1_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history1 = model1.fit(X1_train, \n",
    "                    Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "541/541 [==============================] - 1088s 2s/step - loss: 1.2982 - accuracy: 0.5717 - val_loss: 0.6061 - val_accuracy: 0.8230\n",
    "\n",
    "Epoch 2/5\n",
    "541/541 [==============================] - 1101s 2s/step - loss: 0.4025 - accuracy: 0.8811 - val_loss: 0.5473 - val_accuracy: 0.8672\n",
    "\n",
    "Epoch 3/5\n",
    "541/541 [==============================] - 1017s 2s/step - loss: 0.2330 - accuracy: 0.9353 - val_loss: 0.3290 - val_accuracy: 0.9180\n",
    "\n",
    "Epoch 4/5\n",
    "541/541 [==============================] - 852s 2s/step - loss: 0.1376 - accuracy: 0.9652 - val_loss: 0.5836 - val_accuracy: 0.8464\n",
    "\n",
    "Epoch 5/5\n",
    "541/541 [==============================] - 837s 2s/step - loss: 0.0995 - accuracy: 0.9749 - val_loss: 0.7046 - val_accuracy: 0.8391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_pred = [np.argmax(x) for x in model1.predict(X1_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, Y1_pred, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0      0.925     0.980     0.951       100\n",
    "           1      0.908     0.990     0.947       100\n",
    "           2      1.000     0.880     0.936       100\n",
    "           3      1.000     0.920     0.958       100\n",
    "           4      0.971     1.000     0.985       100\n",
    "           5      0.952     0.990     0.971       100\n",
    "           6      0.935     1.000     0.966       100\n",
    "           7      1.000     0.910     0.953       100\n",
    "    accuracy                          0.959       800\n",
    "    macro avg     0.961     0.959     0.959       800\n",
    "    weighted avg  0.961     0.959     0.959       800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for the train set without NLTK+USPTO+technical stopwords\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X2.shape[1]))\n",
    "model2.add(SpatialDropout1D(0.2))\n",
    "model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(8, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model can be loaded using following line instead of training\n",
    "model2 = keras.models.load_model(data_folder+'model2_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history2 = model2.fit(X2_train, \n",
    "                    Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "541/541 [==============================] - 841s 2s/step - loss: 1.0325 - accuracy: 0.6675 - val_loss: 0.3970 - val_accuracy: 0.9190\n",
    "\n",
    "Epoch 2/5\n",
    "541/541 [==============================] - 1037s 2s/step - loss: 0.3455 - accuracy: 0.8992 - val_loss: 0.6197 - val_accuracy: 0.8019\n",
    "\n",
    "Epoch 3/5\n",
    "541/541 [==============================] - 1451s 3s/step - loss: 0.1996 - accuracy: 0.9468 - val_loss: 0.4184 - val_accuracy: 0.9029\n",
    "\n",
    "Epoch 4/5\n",
    "541/541 [==============================] - 1049s 2s/step - loss: 0.1178 - accuracy: 0.9706 - val_loss: 0.3750 - val_accuracy: 0.9013\n",
    "\n",
    "Epoch 5/5\n",
    "541/541 [==============================] - 892s 2s/step - loss: 0.1051 - accuracy: 0.9735 - val_loss: 0.4060 - val_accuracy: 0.9063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2_pred = [np.argmax(x) for x in model2.predict(X2_test)]\n",
    "print(metrics.classification_report(y_test, Y2_pred, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0      0.990     0.960     0.975       100\n",
    "           1      0.917     0.990     0.952       100\n",
    "           2      1.000     0.960     0.980       100\n",
    "           3      0.989     0.900     0.942       100\n",
    "           4      0.961     0.980     0.970       100\n",
    "           5      0.952     0.990     0.971       100\n",
    "           6      0.962     1.000     0.980       100\n",
    "           7      1.000     0.980     0.990       100\n",
    "\n",
    "    accuracy                          0.970       800\n",
    "    macro avg     0.971     0.970     0.970       800\n",
    "    weighted avg  0.971     0.970     0.970       800"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
